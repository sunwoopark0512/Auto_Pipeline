import os
import json
import time
import logging
import re
from datetime import datetime
from notion_client import Client
from dotenv import load_dotenv

# ---------------------- ì„¤ì • ë¡œë”© ----------------------
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_API_TOKEN")
NOTION_HOOK_DB_ID = os.getenv("NOTION_HOOK_DB_ID")
HOOK_JSON_PATH = os.getenv("HOOK_OUTPUT_PATH", "data/generated_hooks.json")
FAILED_OUTPUT_PATH = "data/upload_failed_hooks.json"
UPLOAD_DELAY = float(os.getenv("UPLOAD_DELAY", "0.5"))

notion = Client(auth=NOTION_TOKEN)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[
        logging.FileHandler("logs/notion_upload.log"),
        logging.StreamHandler()
    ]
)

# ---------------------- ìœ í‹¸: Notion rich_text ì œí•œ ì²˜ë¦¬ ----------------------
def truncate_text(text, max_length=2000):
    return text if len(text) <= max_length else text[:max_length]

# ---------------------- ì¤‘ë³µ í‚¤ì›Œë“œ í™•ì¸ í•¨ìˆ˜ ----------------------
def page_exists(keyword):
    try:
        query = notion.databases.query(
            database_id=NOTION_HOOK_DB_ID,
            filter={"property": "í‚¤ì›Œë“œ", "title": {"equals": keyword}},
            page_size=1
        )
        return len(query.get("results", [])) > 0
    except Exception as e:
        logging.warning(f"âš ï¸ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {keyword} - {e}")
        return False

# ---------------------- GPT ê²°ê³¼ íŒŒì‹± í•¨ìˆ˜ ----------------------
def parse_generated_text(text):
    """Parse JSON returned from GPT and validate its structure."""
    try:
        start = text.find('{')
        end = text.rfind('}') + 1
        if start == -1 or end == -1:
            raise ValueError("JSON object not found")
        data = json.loads(text[start:end])
    except Exception as e:
        logging.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {
            "hook_lines": ["", ""],
            "blog_paragraphs": ["", "", ""],
            "video_titles": ["", ""]
        }

    hook_lines = data.get("hook_lines") if isinstance(data.get("hook_lines"), list) else []
    blog_paragraphs = data.get("blog_paragraphs") if isinstance(data.get("blog_paragraphs"), list) else []
    video_titles = data.get("video_titles") if isinstance(data.get("video_titles"), list) else []

    # Pad lists to expected lengths
    hook_lines = (hook_lines + ["", ""])[:2]
    blog_paragraphs = (blog_paragraphs + ["", "", ""])[:3]
    video_titles = (video_titles + ["", ""])[:2]

    return {
        "hook_lines": hook_lines,
        "blog_paragraphs": blog_paragraphs,
        "video_titles": video_titles
    }

# ---------------------- Notion í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ ----------------------
def create_notion_page(item):
    keyword = item["keyword"]
    parsed = parse_generated_text(item.get("generated_text", ""))
    topic = keyword.split()[0] if " " in keyword else keyword

    notion.pages.create(
        parent={"database_id": NOTION_HOOK_DB_ID},
        properties={
            "í‚¤ì›Œë“œ": {"title": [{"text": {"content": keyword}}]},
            "ì±„ë„": {"select": {"name": topic}},
            "ë“±ë¡ì¼": {"date": {"start": datetime.utcnow().isoformat() + 'Z'}},
            "í›„í‚¹ë¬¸1": {"rich_text": [{"text": {"content": truncate_text(parsed["hook_lines"][0])}}]},
            "í›„í‚¹ë¬¸2": {"rich_text": [{"text": {"content": truncate_text(parsed["hook_lines"][1])}}]},
            "ë¸”ë¡œê·¸ì´ˆì•ˆ": {"rich_text": [{"text": {"content": truncate_text('\n'.join(parsed["blog_paragraphs"]))}}]},
            "ì˜ìƒì œëª©": {"rich_text": [{"text": {"content": truncate_text('\n'.join(parsed["video_titles"]))}}]}
        }
    )

# ---------------------- ì—…ë¡œë“œ ì‹¤í–‰ í•¨ìˆ˜ ----------------------
def upload_all_hooks():
    if not NOTION_TOKEN or not NOTION_HOOK_DB_ID:
        logging.error("â— í™˜ê²½ ë³€ìˆ˜(NOTION_API_TOKEN, NOTION_HOOK_DB_ID)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    try:
        with open(HOOK_JSON_PATH, 'r', encoding='utf-8') as f:
            hooks = json.load(f)
    except Exception as e:
        logging.error(f"â— í›„í‚¹ JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return

    total, success, skipped, failed = 0, 0, 0, 0
    failed_items = []

    for item in hooks:
        keyword = item.get("keyword")
        if not keyword:
            logging.warning("â›” ë¹ˆ í‚¤ì›Œë“œ í•­ëª©, ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        total += 1
        if page_exists(keyword):
            logging.info(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {keyword}")
            skipped += 1
            continue

        for attempt in range(3):
            try:
                create_notion_page(item)
                logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {keyword}")
                success += 1
                break
            except Exception as e:
                logging.warning(f"ğŸ” ì¬ì‹œë„ {attempt+1}/3 - {keyword} | ì˜¤ë¥˜: {e}")
                time.sleep(1)
        else:
            logging.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {keyword}")
            failed_items.append(item)
            failed += 1

        time.sleep(UPLOAD_DELAY)

    if failed_items:
        os.makedirs(os.path.dirname(FAILED_OUTPUT_PATH), exist_ok=True)
        with open(FAILED_OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(failed_items, f, ensure_ascii=False, indent=2)
        logging.info(f"â— ì‹¤íŒ¨ í•­ëª© ì €ì¥ë¨: {FAILED_OUTPUT_PATH}")

    logging.info("ğŸ“Š í›„í‚¹ ì—…ë¡œë“œ ìš”ì•½")
    logging.info(f"ì´ í•­ëª©: {total} | ì„±ê³µ: {success} | ì¤‘ë³µìŠ¤í‚µ: {skipped} | ì‹¤íŒ¨: {failed}")

if __name__ == "__main__":
    upload_all_hooks()
