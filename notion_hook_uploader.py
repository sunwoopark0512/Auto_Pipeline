import os
import json
import time
import logging
from datetime import datetime
from notion_client import Client
from dotenv import load_dotenv

# ---------------------- ÏÑ§Ï†ï Î°úÎî© ----------------------
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_API_TOKEN")
NOTION_HOOK_DB_ID = os.getenv("NOTION_HOOK_DB_ID")
HOOK_JSON_PATH = os.getenv("HOOK_OUTPUT_PATH", "data/generated_hooks.json")
FAILED_OUTPUT_PATH = "data/upload_failed_hooks.json"
UPLOAD_DELAY = float(os.getenv("UPLOAD_DELAY", "0.5"))

notion = Client(auth=NOTION_TOKEN)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[
        logging.FileHandler("logs/notion_upload.log"),
        logging.StreamHandler()
    ]
)

# ---------------------- Ïú†Ìã∏: Notion rich_text Ï†úÌïú Ï≤òÎ¶¨ ----------------------
def truncate_text(text, max_length=2000):
    return text if len(text) <= max_length else text[:max_length]

# ---------------------- Ï§ëÎ≥µ ÌÇ§ÏõåÎìú ÌôïÏù∏ Ìï®Ïàò ----------------------
def page_exists(keyword):
    try:
        query = notion.databases.query(
            database_id=NOTION_HOOK_DB_ID,
            filter={"property": "ÌÇ§ÏõåÎìú", "title": {"equals": keyword}},
            page_size=1
        )
        return len(query.get("results", [])) > 0
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Ï§ëÎ≥µ ÌôïÏù∏ Ïã§Ìå®: {keyword} - {e}")
        return False

# ---------------------- GPT Í≤∞Í≥º ÌååÏã± Ìï®Ïàò ----------------------
def parse_generated_text(text):
    try:
        data = json.loads(text)
        hook_lines = data.get("hook_lines", [])
        blog_paragraphs = data.get("blog_paragraphs", [])
        video_titles = data.get("video_titles", [])

        if not isinstance(hook_lines, list) or len(hook_lines) < 2:
            raise ValueError("hook_lines ÌòïÏãù Ïò§Î•ò")
        if not isinstance(blog_paragraphs, list) or len(blog_paragraphs) < 3:
            raise ValueError("blog_paragraphs ÌòïÏãù Ïò§Î•ò")
        if not isinstance(video_titles, list) or len(video_titles) < 2:
            raise ValueError("video_titles ÌòïÏãù Ïò§Î•ò")

        return {
            "hook_lines": [str(hook_lines[0]), str(hook_lines[1])],
            "blog_paragraphs": [str(p) for p in blog_paragraphs[:3]],
            "video_titles": [str(video_titles[0]), str(video_titles[1])],
        }
    except Exception as e:  # pylint: disable=broad-except
        logging.warning(f"‚ö†Ô∏è JSON ÌååÏã± Ïã§Ìå®: {e}")
        return {
            "hook_lines": ["", ""],
            "blog_paragraphs": ["", "", ""],
            "video_titles": ["", ""],
        }

# ---------------------- Notion ÌéòÏù¥ÏßÄ ÏÉùÏÑ± Ìï®Ïàò ----------------------
def create_notion_page(item):
    keyword = item["keyword"]
    parsed = parse_generated_text(item.get("generated_text", ""))
    topic = keyword.split()[0] if " " in keyword else keyword

    notion.pages.create(
        parent={"database_id": NOTION_HOOK_DB_ID},
        properties={
            "ÌÇ§ÏõåÎìú": {"title": [{"text": {"content": keyword}}]},
            "Ï±ÑÎÑê": {"select": {"name": topic}},
            "Îì±Î°ùÏùº": {"date": {"start": datetime.utcnow().isoformat() + 'Z'}},
            "ÌõÑÌÇπÎ¨∏1": {"rich_text": [{"text": {"content": truncate_text(parsed["hook_lines"][0])}}]},
            "ÌõÑÌÇπÎ¨∏2": {"rich_text": [{"text": {"content": truncate_text(parsed["hook_lines"][1])}}]},
            "Î∏îÎ°úÍ∑∏Ï¥àÏïà": {"rich_text": [{"text": {"content": truncate_text('\n'.join(parsed["blog_paragraphs"]))}}]},
            "ÏòÅÏÉÅÏ†úÎ™©": {"rich_text": [{"text": {"content": truncate_text('\n'.join(parsed["video_titles"]))}}]}
        }
    )

# ---------------------- ÏóÖÎ°úÎìú Ïã§Ìñâ Ìï®Ïàò ----------------------
def upload_all_hooks():
    if not NOTION_TOKEN or not NOTION_HOOK_DB_ID:
        logging.error("‚ùó ÌôòÍ≤Ω Î≥ÄÏàò(NOTION_API_TOKEN, NOTION_HOOK_DB_ID)Í∞Ä ÎàÑÎùΩÎêòÏóàÏäµÎãàÎã§.")
        return

    try:
        with open(HOOK_JSON_PATH, 'r', encoding='utf-8') as f:
            hooks = json.load(f)
    except Exception as e:
        logging.error(f"‚ùó ÌõÑÌÇπ JSON ÌååÏùº ÏùΩÍ∏∞ Ïò§Î•ò: {e}")
        return

    total, success, skipped, failed = 0, 0, 0, 0
    failed_items = []

    for item in hooks:
        keyword = item.get("keyword")
        if not keyword:
            logging.warning("‚õî Îπà ÌÇ§ÏõåÎìú Ìï≠Î™©, Í±¥ÎÑàÎúÅÎãàÎã§.")
            continue

        total += 1
        if page_exists(keyword):
            logging.info(f"‚è≠Ô∏è Ï§ëÎ≥µ Ïä§ÌÇµ: {keyword}")
            skipped += 1
            continue

        for attempt in range(3):
            try:
                create_notion_page(item)
                logging.info(f"‚úÖ ÏóÖÎ°úÎìú ÏôÑÎ£å: {keyword}")
                success += 1
                break
            except Exception as e:
                logging.warning(f"üîÅ Ïû¨ÏãúÎèÑ {attempt+1}/3 - {keyword} | Ïò§Î•ò: {e}")
                time.sleep(1)
        else:
            logging.error(f"‚ùå ÏóÖÎ°úÎìú Ïã§Ìå®: {keyword}")
            failed_items.append(item)
            failed += 1

        time.sleep(UPLOAD_DELAY)

    if failed_items:
        os.makedirs(os.path.dirname(FAILED_OUTPUT_PATH), exist_ok=True)
        with open(FAILED_OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(failed_items, f, ensure_ascii=False, indent=2)
        logging.info(f"‚ùó Ïã§Ìå® Ìï≠Î™© Ï†ÄÏû•Îê®: {FAILED_OUTPUT_PATH}")

    logging.info("üìä ÌõÑÌÇπ ÏóÖÎ°úÎìú ÏöîÏïΩ")
    logging.info(f"Ï¥ù Ìï≠Î™©: {total} | ÏÑ±Í≥µ: {success} | Ï§ëÎ≥µÏä§ÌÇµ: {skipped} | Ïã§Ìå®: {failed}")

if __name__ == "__main__":
    upload_all_hooks()
