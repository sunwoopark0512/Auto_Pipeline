import os
import json
import time
import logging
import re
from datetime import datetime
import asyncio
from notion_client import Client
from dotenv import load_dotenv

# ---------------------- ì„¤ì • ë¡œë”© ----------------------
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_API_TOKEN")
NOTION_HOOK_DB_ID = os.getenv("NOTION_HOOK_DB_ID")
HOOK_JSON_PATH = os.getenv("HOOK_OUTPUT_PATH", "data/generated_hooks.json")
FAILED_OUTPUT_PATH = "data/upload_failed_hooks.json"
UPLOAD_DELAY = float(os.getenv("UPLOAD_DELAY", "0.5"))
CACHE_PATH = os.getenv("HOOK_CACHE_PATH", "data/uploaded_hooks_cache.json")
UPLOAD_WORKERS = int(os.getenv("UPLOAD_WORKERS", "5"))

notion = Client(auth=NOTION_TOKEN)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[
        logging.FileHandler("logs/notion_upload.log"),
        logging.StreamHandler()
    ]
)

# ---------------------- ìºì‹œ ë¡œë”© ----------------------
if os.path.exists(CACHE_PATH):
    with open(CACHE_PATH, 'r', encoding='utf-8') as f:
        uploaded_cache = set(json.load(f))
else:
    uploaded_cache = set()

# ---------------------- ìœ í‹¸: Notion rich_text ì œí•œ ì²˜ë¦¬ ----------------------
def truncate_text(text, max_length=2000):
    return text if len(text) <= max_length else text[:max_length]

# ---------------------- ì¤‘ë³µ í‚¤ì›Œë“œ í™•ì¸ í•¨ìˆ˜ ----------------------
def _page_exists_sync(keyword):
    query = notion.databases.query(
        database_id=NOTION_HOOK_DB_ID,
        filter={"property": "í‚¤ì›Œë“œ", "title": {"equals": keyword}},
        page_size=1
    )
    return len(query.get("results", [])) > 0

async def page_exists(keyword):
    if keyword in uploaded_cache:
        return True
    try:
        exists = await asyncio.to_thread(_page_exists_sync, keyword)
        if exists:
            uploaded_cache.add(keyword)
        return exists
    except Exception as e:
        logging.warning(f"âš ï¸ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {keyword} - {e}")
        return False

# ---------------------- GPT ê²°ê³¼ íŒŒì‹± í•¨ìˆ˜ ----------------------
def parse_generated_text(text):
    hook_lines = re.findall(r"í›„í‚¹ ?ë¬¸ì¥[0-9]?[\s:ï¼š\-\)]*([^\n]+)", text)
    blog_match = re.search(r"ë¸”ë¡œê·¸(?:\s*ì´ˆì•ˆ)?[\s:ï¼š\-\)]*(.*?)\n+\s*(.*?\n+.*?\n+.*?)(?:\n|$)", text, re.DOTALL)
    video_titles = re.findall(r"(?:ì˜ìƒ ì œëª©|YouTube ì œëª©)[\s:ï¼š\-\)]*[^\n]*\n?-\s*(.+)", text)

    blog_paragraphs = [p.strip() for p in blog_match[1].strip().split('\n')[:3]] if blog_match else ["", "", ""]
    return {
        "hook_lines": hook_lines[:2] if len(hook_lines) >= 2 else ["", ""],
        "blog_paragraphs": blog_paragraphs,
        "video_titles": video_titles[:2] if video_titles else ["", ""]
    }

# ---------------------- Notion í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ ----------------------
def create_notion_page(item):
    keyword = item["keyword"]
    parsed = parse_generated_text(item.get("generated_text", ""))
    topic = keyword.split()[0] if " " in keyword else keyword

    notion.pages.create(
        parent={"database_id": NOTION_HOOK_DB_ID},
        properties={
            "í‚¤ì›Œë“œ": {"title": [{"text": {"content": keyword}}]},
            "ì±„ë„": {"select": {"name": topic}},
            "ë“±ë¡ì¼": {"date": {"start": datetime.utcnow().isoformat() + 'Z'}},
            "í›„í‚¹ë¬¸1": {"rich_text": [{"text": {"content": truncate_text(parsed["hook_lines"][0])}}]},
            "í›„í‚¹ë¬¸2": {"rich_text": [{"text": {"content": truncate_text(parsed["hook_lines"][1])}}]},
            "ë¸”ë¡œê·¸ì´ˆì•ˆ": {"rich_text": [{"text": {"content": truncate_text('\n'.join(parsed["blog_paragraphs"]))}}]},
            "ì˜ìƒì œëª©": {"rich_text": [{"text": {"content": truncate_text('\n'.join(parsed["video_titles"]))}}]}
        }
    )

async def create_notion_page_async(item):
    await asyncio.to_thread(create_notion_page, item)

# ---------------------- ì—…ë¡œë“œ ì‹¤í–‰ í•¨ìˆ˜ ----------------------
async def upload_all_hooks():
    if not NOTION_TOKEN or not NOTION_HOOK_DB_ID:
        logging.error("â— í™˜ê²½ ë³€ìˆ˜(NOTION_API_TOKEN, NOTION_HOOK_DB_ID)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    try:
        with open(HOOK_JSON_PATH, 'r', encoding='utf-8') as f:
            hooks = json.load(f)
    except Exception as e:
        logging.error(f"â— í›„í‚¹ JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return

    sem = asyncio.Semaphore(UPLOAD_WORKERS)
    results = []

    async def process_item(item):
        keyword = item.get("keyword")
        if not keyword:
            logging.warning("â›” ë¹ˆ í‚¤ì›Œë“œ í•­ëª©, ê±´ë„ˆëœë‹ˆë‹¤.")
            return "skip", None

        async with sem:
            if await page_exists(keyword):
                logging.info(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {keyword}")
                await asyncio.sleep(UPLOAD_DELAY)
                return "skip", None

            for attempt in range(3):
                try:
                    await create_notion_page_async(item)
                    uploaded_cache.add(keyword)
                    logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {keyword}")
                    await asyncio.sleep(UPLOAD_DELAY)
                    return "success", None
                except Exception as e:
                    logging.warning(f"ğŸ” ì¬ì‹œë„ {attempt+1}/3 - {keyword} | ì˜¤ë¥˜: {e}")
                    await asyncio.sleep(1)

            logging.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {keyword}")
            await asyncio.sleep(UPLOAD_DELAY)
            return "failed", item

    tasks = [process_item(item) for item in hooks]
    results = await asyncio.gather(*tasks)

    total = len(hooks)
    success = sum(1 for r, _ in results if r == "success")
    skipped = sum(1 for r, _ in results if r == "skip")
    failed_items = [item for r, item in results if r == "failed"]
    failed = len(failed_items)

    if failed_items:
        os.makedirs(os.path.dirname(FAILED_OUTPUT_PATH), exist_ok=True)
        with open(FAILED_OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(failed_items, f, ensure_ascii=False, indent=2)
        logging.info(f"â— ì‹¤íŒ¨ í•­ëª© ì €ì¥ë¨: {FAILED_OUTPUT_PATH}")

    if uploaded_cache:
        os.makedirs(os.path.dirname(CACHE_PATH), exist_ok=True)
        with open(CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(list(uploaded_cache), f, ensure_ascii=False, indent=2)

    logging.info("ğŸ“Š í›„í‚¹ ì—…ë¡œë“œ ìš”ì•½")
    logging.info(f"ì´ í•­ëª©: {total} | ì„±ê³µ: {success} | ì¤‘ë³µìŠ¤í‚µ: {skipped} | ì‹¤íŒ¨: {failed}")

if __name__ == "__main__":
    asyncio.run(upload_all_hooks())
