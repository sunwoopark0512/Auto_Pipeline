# Auto Pipeline

This repository contains a collection of Python scripts that gather trending keywords, generate marketing hooks with GPT, and upload the results to Notion. The scripts can be run individually or orchestrated through `run_pipeline.py` or a GitHub Actions workflow.

## Dependencies

- Python 3.10+
- `openai`
- `python-dotenv`
- `notion-client`
- `pytrends`
- `snscrape`

Install dependencies with:

```bash
pip install openai python-dotenv notion-client pytrends snscrape
```

## Environment Variables

Create a `.env` file in the project root and define the variables required by the scripts:

```
OPENAI_API_KEY=<your-openai-key>
NOTION_API_TOKEN=<your-notion-token>
NOTION_DB_ID=<keyword database id>
NOTION_HOOK_DB_ID=<hook database id>
NOTION_KPI_DB_ID=<kpi database id>
KEYWORD_OUTPUT_PATH=data/keyword_output_with_cpc.json
HOOK_OUTPUT_PATH=data/generated_hooks.json
FAILED_HOOK_PATH=logs/failed_hooks.json
REPARSED_OUTPUT_PATH=logs/failed_keywords_reparsed.json
UPLOAD_DELAY=0.5
RETRY_DELAY=0.5
API_DELAY=1.0
TOPIC_CHANNELS_PATH=config/topic_channels.json
UPLOADED_CACHE_PATH=data/uploaded_keywords_cache.json
FAILED_UPLOADS_PATH=logs/failed_uploads.json
```

Adjust paths and delays as needed.

## Script Overview

- **`keyword_auto_pipeline.py`** ‚Äì Collects trending keywords from Google Trends and Twitter and filters them. Output is saved to `KEYWORD_OUTPUT_PATH`.
- **`hook_generator.py`** ‚Äì Uses GPT to generate hooks for keywords in the previous output. Results are stored in `HOOK_OUTPUT_PATH`. Failed generations are written to `FAILED_HOOK_PATH`.
- **`notion_hook_uploader.py`** ‚Äì Parses the generated hooks and uploads them to the Notion database defined by `NOTION_HOOK_DB_ID`.
- **`scripts/notion_uploader.py`** ‚Äì Uploads the filtered keywords themselves to Notion using `NOTION_DB_ID`.
- **`retry_failed_uploads.py`** / **`scripts/retry_failed_uploads.py`** ‚Äì Retry uploading failed items recorded in `REPARSED_OUTPUT_PATH` or `FAILED_HOOK_PATH`.
- **`retry_dashboard_notifier.py`** ‚Äì Summarises retry results and posts KPI data to a Notion dashboard using `NOTION_KPI_DB_ID`.
- **`run_pipeline.py`** ‚Äì Simple orchestrator that executes a sequence of scripts.

## Running Manually

Activate your virtual environment, ensure the `.env` file is configured, then execute:

```bash
python run_pipeline.py
```

Example output when dependencies are missing:

```text
2025-06-16 00:56:17,103 INFO:üß© ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë: 2025-06-16 00:56
2025-06-16 00:56:17,103 ERROR:‚ùå ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: scripts/hook_generator.py
...
```

Each script can also be run individually for testing.

## GitHub Actions

A sample workflow is provided in `.github/workflows/daily-pipeline.yml.txt`. Rename it to `.yml` and push to enable scheduled runs. The workflow installs dependencies and runs `python scripts/run_pipeline.py`. Secret values such as `OPENAI_API_KEY` and Notion tokens should be added to the repository secrets.

