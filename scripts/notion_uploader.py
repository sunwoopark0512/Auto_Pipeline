import os
import json
import time
import logging
from datetime import datetime
from notion_client import Client
from dotenv import load_dotenv
from jsonschema import ValidationError, validate
from schemas import KEYWORD_OUTPUT_SCHEMA

# ---------------------- ì„¤ì • ë¡œë”© ----------------------
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_API_TOKEN")
NOTION_DB_ID = os.getenv("NOTION_DB_ID")
KEYWORD_JSON_PATH = os.getenv("KEYWORD_OUTPUT_PATH", "data/keyword_output_with_cpc.json")
UPLOAD_DELAY = float(os.getenv("UPLOAD_DELAY", "0.5"))
CACHE_PATH = os.getenv("UPLOADED_CACHE_PATH", "data/uploaded_keywords_cache.json")
FAILED_PATH = os.getenv("FAILED_UPLOADS_PATH", "logs/failed_uploads.json")

# ---------------------- ë¡œê¹… ì„¤ì • ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')

# ---------------------- Notion í´ë¼ì´ì–¸íŠ¸ ----------------------
notion = Client(auth=NOTION_TOKEN)

# ---------------------- ìºì‹œ ë¡œë”© ----------------------
if os.path.exists(CACHE_PATH):
    with open(CACHE_PATH, 'r', encoding='utf-8') as f:
        uploaded_cache = set(json.load(f))
else:
    uploaded_cache = set()

failed_uploads = []

# ---------------------- ì¤‘ë³µ í‚¤ì›Œë“œ í™•ì¸ í•¨ìˆ˜ ----------------------
def page_exists(keyword):
    if keyword in uploaded_cache:
        return True
    try:
        query = notion.databases.query(
            database_id=NOTION_DB_ID,
            filter={"property": "í‚¤ì›Œë“œ", "title": {"equals": keyword}},
            page_size=1
        )
        return len(query.get("results", [])) > 0
    except Exception as e:
        logging.warning(f"âš ï¸ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {keyword} - {e}")
        return False

# ---------------------- Notion í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ ----------------------
def create_notion_page(item):
    topic = item['keyword'].split()[0]  # ì²« ë‹¨ì–´ë¥¼ ì£¼ì œ ì±„ë„ë¡œ í™œìš©

    notion.pages.create(
        parent={"database_id": NOTION_DB_ID},
        properties={
            "í‚¤ì›Œë“œ": {"title": [{"text": {"content": item['keyword']}}]},
            "ì¶œì²˜": {"select": {"name": item['source']}},
            "ì±„ë„": {"select": {"name": topic}},
            "ê²€ìƒ‰ëŸ‰": {"number": item.get("score", 0)},
            "ì„±ì¥ë¥ ": {"number": item.get("growth", 0)},
            "ë©˜ì…˜ìˆ˜": {"number": item.get("mentions", 0)},
            "ìµœëŒ€ë¦¬íŠ¸ìœ—": {"number": item.get("top_retweet", 0)},
            "CPC(ì›)": {"number": item.get("cpc", 0)},
            "ë“±ë¡ì¼": {"date": {"start": datetime.utcnow().isoformat() + 'Z'}}
        }
    )

# ---------------------- ì—…ë¡œë“œ ë©”ì¸ í•¨ìˆ˜ ----------------------
def upload_all_keywords():
    if not NOTION_TOKEN or not NOTION_DB_ID:
        logging.error("â— í™˜ê²½ ë³€ìˆ˜(NOTION_API_TOKEN, NOTION_DB_ID)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    try:
        with open(KEYWORD_JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        validate(data, KEYWORD_OUTPUT_SCHEMA)
        keywords = data.get("filtered_keywords", [])
    except (json.JSONDecodeError, ValidationError) as e:
        logging.error(f"â— í‚¤ì›Œë“œ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return

    total = len(keywords)
    success, skipped, failed = 0, 0, 0

    for item in keywords:
        keyword = item.get('keyword')
        if not keyword:
            logging.warning("â›” ë¹ˆ í‚¤ì›Œë“œ í•­ëª© ë°œê²¬, ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        if page_exists(keyword):
            logging.info(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {keyword}")
            skipped += 1
            continue

        for attempt in range(3):
            try:
                create_notion_page(item)
                uploaded_cache.add(keyword)
                logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {keyword}")
                success += 1
                time.sleep(UPLOAD_DELAY)
                break
            except Exception as e:
                logging.warning(f"ğŸ” ì¬ì‹œë„ {attempt + 1}/3 - {keyword} | ì˜¤ë¥˜: {e}")
                time.sleep(1)
        else:
            logging.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {keyword} | ë°ì´í„°: {item}")
            failed_uploads.append(item)
            failed += 1

    # ìºì‹œ ì €ì¥
    try:
        os.makedirs(os.path.dirname(CACHE_PATH), exist_ok=True)
        with open(CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(list(uploaded_cache), f, ensure_ascii=False, indent=2)
        logging.info(f"ğŸ“¦ ì—…ë¡œë“œ ìºì‹œ ì €ì¥ ì™„ë£Œ: {CACHE_PATH}")
    except Exception as e:
        logging.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
    if failed_uploads:
        try:
            os.makedirs(os.path.dirname(FAILED_PATH), exist_ok=True)
            with open(FAILED_PATH, 'w', encoding='utf-8') as f:
                json.dump(failed_uploads, f, ensure_ascii=False, indent=2)
            logging.info(f"â— ì‹¤íŒ¨ í•­ëª© ê¸°ë¡ ì™„ë£Œ: {FAILED_PATH}")
        except Exception as e:
            logging.warning(f"âš ï¸ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ---------------------- ìš”ì•½ ê²°ê³¼ ì¶œë ¥ ----------------------
    logging.info("ğŸ¯ ì—…ë¡œë“œ ì™„ë£Œ ìš”ì•½")
    logging.info(f"ì´ í‚¤ì›Œë“œ: {total} | ì„±ê³µ: {success} | ì¤‘ë³µìŠ¤í‚µ: {skipped} | ì‹¤íŒ¨: {failed}")

# ---------------------- ë©”ì¸ ì§„ì…ì  ----------------------
if __name__ == "__main__":
    upload_all_keywords()
