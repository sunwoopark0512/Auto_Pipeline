import os
import json
import logging
import re
from dotenv import load_dotenv

load_dotenv()

FAILED_HOOK_PATH = os.getenv("FAILED_HOOK_PATH", "logs/failed_hooks.json")
REPARSED_OUTPUT_PATH = os.getenv("REPARSED_OUTPUT_PATH", "logs/failed_keywords_reparsed.json")

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')


def parse_generated_text(text: str) -> dict:
    """Extract structured fields from GPT output text.

    The function first attempts a regex based extraction. If that fails,
    it falls back to a simple line split approach similar to the one used
    during generation.
    """

    hook_lines: list[str] = []
    blog_paragraphs: list[str] = []
    video_titles: list[str] = []
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    section = "hook"
    for line in lines:
        if line.startswith("ë¸”ë¡œê·¸"):
            section = "blog"
            continue
        if "ì˜ìƒ" in line or "YouTube" in line:
            section = "video"
            continue

        if section == "hook" and len(hook_lines) < 2:
            cleaned = re.sub(r"í›„í‚¹ ?ë¬¸ì¥[0-9]?[\s:ï¼š\-\)]*", "", line)
            hook_lines.append(cleaned)
        elif section == "blog" and len(blog_paragraphs) < 3:
            blog_paragraphs.append(line)
        elif section == "video" and len(video_titles) < 2:
            video_titles.append(line)

    return {
        "hook_lines": (hook_lines + ["", ""])[:2],
        "blog_paragraphs": (blog_paragraphs + ["", "", ""])[:3],
        "video_titles": (video_titles + ["", ""])[:2],
    }


def reparse_failed_gpt() -> None:
    if not os.path.exists(FAILED_HOOK_PATH):
        logging.error(f"âŒ ì‹¤íŒ¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {FAILED_HOOK_PATH}")
        return

    with open(FAILED_HOOK_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    logging.info(f"ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ì‹¤íŒ¨ í•­ëª©: {len(data)}ê°œ")

    reparsed = []
    for item in data:
        keyword = item.get("keyword")
        if not keyword:
            logging.warning("â›” keyword ëˆ„ë½ í•­ëª© ê±´ë„ˆëœ€")
            continue
        text = item.get("generated_text", "") or ""
        parsed = parse_generated_text(text)
        reparsed.append({
            "keyword": keyword,
            "hook_lines": parsed["hook_lines"],
            "blog_paragraphs": parsed["blog_paragraphs"],
            "video_titles": parsed["video_titles"],
        })
        logging.debug(f"âœ… íŒŒì‹± ì™„ë£Œ: {keyword}")

    os.makedirs(os.path.dirname(REPARSED_OUTPUT_PATH), exist_ok=True)
    with open(REPARSED_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(reparsed, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… ì¬íŒŒì‹± ê²°ê³¼ ì €ì¥: {REPARSED_OUTPUT_PATH}")
    logging.info(f"ì´ íŒŒì‹± ì„±ê³µ: {len(reparsed)}")


if __name__ == "__main__":
    reparse_failed_gpt()
