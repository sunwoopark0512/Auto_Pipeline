import os
import json
import logging
import re
from dotenv import load_dotenv

# ---------------------- ì„¤ì • ë¡œë”© ----------------------
load_dotenv()
FAILED_HOOK_PATH = os.getenv("FAILED_HOOK_PATH", "logs/failed_hooks.json")
OUTPUT_PATH = os.getenv("REPARSED_OUTPUT_PATH", "logs/failed_keywords_reparsed.json")

# ---------------------- ë¡œê¹… ì„¤ì • ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')


# ---------------------- GPT ì¶œë ¥ íŒŒì‹± ----------------------
def parse_generated_text(text: str) -> dict:
    """Parse GPT generated text into hook lines, blog paragraphs and video titles."""
    hook_lines = re.findall(r"í›„í‚¹ ?ë¬¸ì¥[0-9]?[\s:ï¼š\-\)]*([^\n]+)", text)
    blog_match = re.search(r"ë¸”ë¡œê·¸(?:\s*ì´ˆì•ˆ)?[\s:ï¼š\-\)]*(.*?)\n+\s*(.*?\n+.*?\n+.*?)(?:\n|$)", text, re.DOTALL)
    video_titles = re.findall(r"(?:ì˜ìƒ ì œëª©|YouTube ì œëª©)[\s:ï¼š\-\)]*[^\n]*\n?-\s*(.+)", text)

    blog_paragraphs = [p.strip() for p in blog_match[1].strip().split('\n')[:3]] if blog_match else ["", "", ""]
    return {
        "hook_lines": hook_lines[:2] if len(hook_lines) >= 2 else ["", ""],
        "blog_paragraphs": blog_paragraphs,
        "video_titles": video_titles[:2] if video_titles else ["", ""]
    }


# ---------------------- ë©”ì¸ íŒŒì‹± í•¨ìˆ˜ ----------------------
def parse_failed_gpt() -> None:
    if not os.path.exists(FAILED_HOOK_PATH):
        logging.error(f"âŒ ì‹¤íŒ¨ í›„í‚¹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FAILED_HOOK_PATH}")
        return

    with open(FAILED_HOOK_PATH, 'r', encoding='utf-8') as f:
        failed_data = json.load(f)

    reparsed = []
    skipped = 0

    for entry in failed_data:
        keyword = entry.get("keyword")
        text = entry.get("generated_text")
        if not keyword or not text:
            logging.warning(f"â›” ë°ì´í„° ëˆ„ë½ - keyword: {keyword} text: {bool(text)}")
            skipped += 1
            continue
        parsed = parse_generated_text(text)
        reparsed.append({
            "keyword": keyword,
            "hook_lines": parsed["hook_lines"],
            "blog_paragraphs": parsed["blog_paragraphs"],
            "video_titles": parsed["video_titles"]
        })

    if reparsed:
        os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
        with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(reparsed, f, ensure_ascii=False, indent=2)
        logging.info(f"âœ… íŒŒì‹± ê²°ê³¼ ì €ì¥: {OUTPUT_PATH}")
    else:
        logging.info("âš ï¸ íŒŒì‹± ê°€ëŠ¥í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    logging.info(f"ğŸ“Š íŒŒì‹± ìš”ì•½ - ì´ {len(failed_data)}ê°œ ì¤‘ {len(reparsed)}ê°œ ì„±ê³µ, {skipped}ê°œ ê±´ë„ˆëœ€")


if __name__ == "__main__":
    parse_failed_gpt()
