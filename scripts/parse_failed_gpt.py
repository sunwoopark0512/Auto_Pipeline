import os
import json
import logging
import re
from dotenv import load_dotenv

# ---------------------- í™˜ê²½ ë³€ìˆ˜ ë¡œë”© ----------------------
load_dotenv()
FAILED_HOOK_PATH = os.getenv("FAILED_HOOK_PATH", "logs/failed_hooks.json")
REPARSED_OUTPUT_PATH = os.getenv("REPARSED_OUTPUT_PATH", "logs/failed_keywords_reparsed.json")

# ---------------------- ë¡œê¹… ì„¤ì • ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')

# ---------------------- GPT ê²°ê³¼ íŒŒì‹± ----------------------
def parse_generated_text(text: str):
    """Extract hook lines, blog paragraphs and video titles from GPT text."""
    hook_lines = re.findall(r"í›„í‚¹ ?ë¬¸ì¥[0-9]?[\s:ï¼š\-\)]*([^\n]+)", text)
    blog_match = re.search(
        r"ë¸”ë¡œê·¸(?:\s*ì´ˆì•ˆ)?[\s:ï¼š\-\)]*(.*?)\n+\s*(.*?\n+.*?\n+.*?)(?:\n|$)",
        text,
        re.DOTALL,
    )
    video_titles = re.findall(r"(?:ì˜ìƒ ì œëª©|YouTube ì œëª©)[\s:ï¼š\-\)]*[^\n]*\n?-\s*(.+)", text)
    blog_paragraphs = (
        [p.strip() for p in blog_match[1].strip().split("\n")[:3]]
        if blog_match
        else ["", "", ""]
    )
    return {
        "hook_lines": hook_lines[:2] if len(hook_lines) >= 2 else ["", ""],
        "blog_paragraphs": blog_paragraphs,
        "video_titles": video_titles[:2] if video_titles else ["", ""],
    }


# ---------------------- ë©”ì¸ ë¡œì§ ----------------------
def parse_failed_gpt():
    """Read failed GPT hooks, parse text and save structured output."""
    if not os.path.exists(FAILED_HOOK_PATH):
        logging.error(f"âŒ ì‹¤íŒ¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {FAILED_HOOK_PATH}")
        return

    with open(FAILED_HOOK_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    parsed_results = []
    success, failed = 0, 0
    for item in data:
        keyword = item.get("keyword", "")
        raw_text = item.get("generated_text", "")
        if not raw_text:
            logging.warning(f"â›” generated_text ì—†ìŒ, ê±´ë„ˆëœ€: {keyword}")
            failed += 1
            continue
        parsed = parse_generated_text(raw_text)
        parsed_results.append({
            "keyword": keyword,
            "hook_lines": parsed["hook_lines"],
            "blog_paragraphs": parsed["blog_paragraphs"],
            "video_titles": parsed["video_titles"],
        })
        success += 1

    os.makedirs(os.path.dirname(REPARSED_OUTPUT_PATH), exist_ok=True)
    with open(REPARSED_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(parsed_results, f, ensure_ascii=False, indent=2)

    logging.info("ğŸ“‘ ì‹¤íŒ¨ GPT ê²°ê³¼ ì¬íŒŒì‹± ì™„ë£Œ")
    logging.info(
        f"ì´ í•­ëª©: {len(data)} | íŒŒì‹± ì„±ê³µ: {success} | íŒŒì‹± ì‹¤íŒ¨: {failed} | ì €ì¥ ê²½ë¡œ: {REPARSED_OUTPUT_PATH}"
    )


if __name__ == "__main__":
    parse_failed_gpt()
